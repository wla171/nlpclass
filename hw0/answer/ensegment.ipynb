{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task uses simple word counts to segment strings into the most likely sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The score reachs 1.00 for dev.txt, 0.97 for test.txt respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "To check the results of input \"data/input/test.txt\", we have manually segmented the string in \"data/reference/test.out\" as a reference."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "how to break up in 5 words\n",
    "what makes god smile\n",
    "10 people who mean a lot to me\n",
    "worst day in 4 words\n",
    "love story in 5 words\n",
    "top 3 favourite comics\n",
    "10 breakup lines\n",
    "things that make you smile\n",
    "best female athlete\n",
    "worst boss in 5 words\n",
    "now is the time for all good\n",
    "it is a truth universally acknowledged\n",
    "when in the course of human events it becomes necessary\n",
    "it was a bright cold day in april and the clocks were striking thirteen\n",
    "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
    "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
    "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sit down on or to eat it was a hobbit hole and that means comfort\n",
    "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small unregarded yellow sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part sets dev.txt as input and prints out the segmentation result. First, import segmentation and support functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensegment import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define token number, instantiate class Pdist and Segment, print out the segmented results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "N = 1024908267229\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\", sep='\\t'),N=N, missingfn = reduce_probability_for_long_word)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to check.py, the score is 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part sets test.txt as input and prints out the segmentation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "N = 1024908267229\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\", sep='\\t'),N=N, missingfn = reduce_probability_for_long_word)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to check.py, the score is 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reaching the final score as shown above, we have tried the default solution and an improved solution as well. After analyzing the output of the previous solutions, a final solution is then introduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Defaut Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below lists the segmented output given by the default solution for dev.txt:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "choose spain\n",
    "this is a test\n",
    "who represents\n",
    "experts exchange\n",
    "speed of art\n",
    "unclimatechangebody\n",
    "we are the people\n",
    "mentionyourfaves\n",
    "now playing\n",
    "the walking dead\n",
    "follow me\n",
    "we are the people\n",
    "mentionyourfaves\n",
    "check domain\n",
    "big rock\n",
    "name cheap\n",
    "apple domains\n",
    "honesty hour\n",
    "being human\n",
    "follow back\n",
    "social media\n",
    "30secondstoearth\n",
    "current ratesoughttogodown\n",
    "this is insane\n",
    "what is my name\n",
    "is it time\n",
    "let us go\n",
    "me too\n",
    "nowthatcherisdead\n",
    "advice for young journalists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to check.py, the score is 0.82 for dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, most strings with 2 words are segmented correctly, but long strings like \"unclimatechangebody\", \"mentionyourfaves\", \"30secondstoearth\" remains unsegmented due to the reason that the product of the P(w) of possible segemented words (e.g. \"un\", \"climate\",\" \"change\", \"body\") is less than the probabilty of a unseen word like \"mentionyourfaves\", which is 1/N given by the default solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same issue occurs with input test.txt,  below is the segmentation given by default solution for test.txt:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "howtobreakupin5words\n",
    "whatmakesgodsmile\n",
    "10peoplewhomeanalot to me\n",
    "worstdayin4words\n",
    "lovestoryin5words\n",
    "top3favouritecomics\n",
    "10breakuplines\n",
    "things thatmakeyousmile\n",
    "bestfemaleathlete\n",
    "worstbossin5words\n",
    "no wisthetimeforallgood\n",
    "i tisatruthuniversally acknowledged\n",
    "when in the course of humaneventsitbecomes necessary\n",
    "itwasabrightcoldda yinaprilandtheclocks werestrikingthirteen\n",
    "it wasthebestoftimesitw astheworstoftimesitw astheageofwisdomitwa stheageoffoolishness\n",
    "asgregorsamsaawo keonemorningfromunea sydreamshefoundhimse lftransformedinhisbe dintoagiganticinsect\n",
    "inaholeinthegroundth erelivedahobbitnotan astydirtywetholefill edwiththeendsofworms and anoozysmellnoryetadr ybaresandyholewithno thinginittositdownon or toeatitwasahobbithol eandthatmeanscomfort\n",
    "faroutintheuncharted backwaters of the unfashionable endofthewesternspira larmofthegalaxyliesa small unregardedyellowsun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to check.py, the score is 0.13 for test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle this issue, we have to decrease the probability of unseen words. In addition, with the length of the unseen words growing, like in \"top3favouritecomics\", some words that do exist in the Corpus will be included in the unseen word, which decreases the chance of finding the right segmentation. So, instead of giving every unseen word the same probabilty, we decrease of the probability of an unseen word according to the length of the word, which leads to the improved solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Improved Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first take a look at the code in default.py, which explains how the probability of a unseen word is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, missingfn is responsible to generate the probability of an unseen word, which assigns a constant 1./N to every unseen word. So we add a new function reduce_probability_for_long_word(word, N) to replace missingfn which decrease the probabilty of an unseen word by (100000 **len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_probability_for_long_word(word, N):\n",
    "\treturn 1./(N * 100000 **len(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the reduce_probability_for_long_word(word, N) function with input dev.txt, we get the outputs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "bigrock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "N = 1024908267229\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\", sep='\\t'),N=N, missingfn = reduce_probability_for_long_word)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to check.py, the score is 0.98. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, only one word is not correctly segmented, i.e. \"bigrock\" is not segmented into \"big rock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the reduce_probability_for_long_word(word, N) function with input test.txt, we get the outputs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howto breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "N = 1024908267229\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\", sep='\\t'),N=N, missingfn = reduce_probability_for_long_word)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to check.py, the score is 0.96."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the output, 5 words are not correctly segmented:\n",
    "\n",
    "   wrong seg.  correct seg.\n",
    "1. howto       how to\n",
    "2. breakup     break up\n",
    "3. alot        a lot\n",
    "4. sitdown     sit down\n",
    "5. un regarded unregarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases 1-4 like \"break up\" and \"breakup\" both have apperance in the Corpus, so we can not solve this by improving the missingfn. Given a  Corpus, the unigram probability of \"breakup\" and the product of \"break\"&\"up\" is a fixed number. In this particular task, to seperate phrases 1-4, we can decrease the probability of word according to its length so that the P(w) product of short words like \"break\"&\"up\" can surpass that of \"breakup\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase 5 \"unregarded\" has no appearance in the Corpus. The P(w) product of \"un\"&\"regarded\" is larger than that of the unseen word, so unregarded is not segmented correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Final Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a word in the Corpus, we change its P(w) from count/N to count/N/(len(w)/3). If the length of a word is less than 3, it's P(w) will be increased; If the length of a word is larger than 3, its P(w) will be decreased, thus seperating short words from phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        if key in self: \n",
    "            return self[key]/(self.N *len(key)/3)\n",
    "        else:\n",
    "            return self.missingfn(key, self.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this idea, we get the final results as shown in the \"Dev Input\" and \"Test Input\" section, reaching a score of 1.00 and 0.97 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dev.txt, the strings are segmented perfectly. For test.txt, the outputs are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "N = 1024908267229\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\", sep='\\t'),N=N, missingfn = reduce_probability_for_long_word)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"howto\" is correctly segmented this time. For dev.txt, \"bigrock\" is correctly segmented into \"big rock\". For other phrases, there is still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this task, we have applied two methods to improve segmentation.\n",
    "\n",
    "1. Decrease the probability of an unseen word according to its length\n",
    "2. Modify the probability of a seen word according to its length\n",
    "\n",
    "After applying these two approaches, we have improved the score from 0.82 to 1.00 for dev.txt. and from 0.13 to 0.97 for test.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
