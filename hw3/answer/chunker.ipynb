{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrasal chunking is the task of finding non-recursive syntactic groups of words.\n",
    "\n",
    "By implementing the Baseline solution, we have achieved a FB1 score of 77.11; By making further improvement, we reached a FB1 score of 78.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Baseline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with noisy input, we implemented a semi-character RNN as guided by the homework instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Character Level Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a character level representation of the word. To achive this, we add a function create_char_level_vector into the default solution. And the final character level representation is the concatenation of the three vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_level_vectors(sentence, word_to_ix, char_to_ix):\n",
    "    width = len(char_to_ix)\n",
    "    vectors = torch.tensor(0)\n",
    "    first = True\n",
    "    for word in sentence:\n",
    "        v1 = torch.zeros([width])\n",
    "        v2 = torch.zeros([width])\n",
    "        v3 = torch.zeros([width])\n",
    "        \"1. Create a one-hot vector v1 for the first character of the word.\"\n",
    "        v1[char_to_ix[word[0]]] = 1\n",
    "\n",
    "        \"2. Create a vector v2 where the index of a character has the count of that character in the word.\"\n",
    "        for c in word[1:-1]:\n",
    "            v2[char_to_ix[c]] = v2[char_to_ix[c]] + 1\n",
    "\n",
    "        \"3. Create a one-hot vector v3 for the last character of the word.\"\n",
    "        v3[char_to_ix[word[-1]]] = 1\n",
    "\n",
    "        \"4. Concatenate three vectors to get final character level representation.\"\n",
    "        v = torch.cat((v1,v2,v3),dim=0).view(1,300)\n",
    "        if first:\n",
    "            vectors = v\n",
    "            first = False\n",
    "        else:\n",
    "            vectors = torch.cat((vectors,v),dim=0)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Concatenate to the word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created a character level representation of the word, we use the first method to combine the semi-Character RNN idea with phrasal chunker, i.e.concatenate to the word embedding input to the chunker RNN an input vector that is the character level representation of the word. Since the character level representation vector length (char_vector_dim) is 300 (v1,v2,v3), so we need to increase the length from word_embedding + char_vector_dim in LSTM input. Now, the sentence object now contains both sentence[0] word vector and sentence[1] char level vector for the senetences\n",
    "\n",
    "To achive this, we modified the class LSTMTaggerModel:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Original Model definition:\n",
    "\n",
    "LSTMTaggerModel(\n",
    "  (word_embeddings): Embedding(9675, 128)\n",
    "  (lstm): LSTM(128, 64)\n",
    "  (hidden2tag): Linear(in_features=64, out_features=22, bias=True)\n",
    ")\n",
    "\n",
    "New Model definition:\n",
    "\n",
    "LSTMTaggerModel(\n",
    "  (word_embeddings): Embedding(9675, 128)\n",
    "  (lstm): LSTM(428, 64)\n",
    "  (hidden2tag): Linear(in_features=64, out_features=22, bias=True)\n",
    "):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        torch.manual_seed(1)\n",
    "        super(LSTMTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # concatenate vector before the LSTM, so the size increase from 128 to 128 + 3 * 100 = 428\n",
    "        # where 128 is the embedding_dim and number of len(string.printable) which is 100 characters with 3 vectors v1, v2, v3\n",
    "        self.lstm = nn.LSTM(embedding_dim+300, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, char_vectors):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "\n",
    "        # concatenate embedding vector with character level vector before lstm\n",
    "        embeds = torch.cat((embeds,char_vectors), dim=1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have concatenated to the word embedding, we have to make corresponding changes in the class LSTMTagger, which use the create_char_level_vector() and the input of self.model() in train():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger:\n",
    "\n",
    "    def __init__(self, trainfile, modelfile, modelsuffix, unk=\"[UNK]\", epochs=10, embedding_dim=128, hidden_dim=64):\n",
    "        self.unk = unk\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.modelfile = modelfile\n",
    "        self.modelsuffix = modelsuffix\n",
    "        self.training_data = []\n",
    "        if trainfile[-3:] == '.gz':\n",
    "            with gzip.open(trainfile, 'rt') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "        else:\n",
    "            with open(trainfile, 'r') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "\n",
    "        self.word_to_ix = {} # replaces words with an index (one-hot vector)\n",
    "        self.char_to_ix = {} # replaces character with an index\n",
    "        self.tag_to_ix = {} # replace output labels / tags with an index\n",
    "        self.ix_to_tag = [] # during inference we produce tag indices so we have to map it back to a tag\n",
    "\n",
    "        for sent, tags in self.training_data:\n",
    "            for word in sent:\n",
    "                if word not in self.word_to_ix:\n",
    "                    self.word_to_ix[word] = len(self.word_to_ix)\n",
    "            for tag in tags:\n",
    "                if tag not in self.tag_to_ix:\n",
    "                    self.tag_to_ix[tag] = len(self.tag_to_ix)\n",
    "                    self.ix_to_tag.append(tag)\n",
    "\n",
    "        for c in string.printable:\n",
    "            self.char_to_ix[c] = string.printable.find(c)\n",
    "\n",
    "        logging.info(\"char_to_ix:\", self.char_to_ix)\n",
    "        logging.info(\"word_to_ix:\", self.word_to_ix)\n",
    "        logging.info(\"tag_to_ix:\", self.tag_to_ix)\n",
    "        logging.info(\"ix_to_tag:\", self.ix_to_tag)\n",
    "\n",
    "        self.model = LSTMTaggerModel(self.embedding_dim, self.hidden_dim, len(self.word_to_ix), len(self.tag_to_ix))\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "    def argmax(self, seq):\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(seq, self.word_to_ix, self.unk)\n",
    "            char_vectors = create_char_level_vectors(seq, self.word_to_ix, self.char_to_ix)\n",
    "            tag_scores = self.model(inputs,char_vectors)\n",
    "            for i in range(len(inputs)):\n",
    "                output.append(self.ix_to_tag[int(tag_scores[i].argmax(dim=0))])\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            # random.shuffle(generate_noise(self.training_data))\n",
    "            for sentence, tags in tqdm.tqdm(self.training_data):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "\n",
    "                # Step 2.1. Create character level vector representation of size n x 300\n",
    "                char_vectors = create_char_level_vectors(sentence, self.word_to_ix, self.char_to_ix)\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = self.model(sentence_in, char_vectors)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                    }, savefile)\n",
    "\n",
    "    def decode(self, inputfile):\n",
    "        if inputfile[-3:] == '.gz':\n",
    "            with gzip.open(inputfile, 'rt') as f:\n",
    "                input_data = read_conll(f, input_idx=0, label_idx=-1)\n",
    "        else:\n",
    "            with open(inputfile, 'r') as f:\n",
    "                input_data = read_conll(f, input_idx=0, label_idx=-1)\n",
    "\n",
    "        if not os.path.isfile(self.modelfile + self.modelsuffix):\n",
    "            raise IOError(\"Error: missing model file {}\".format(self.modelfile + self.modelsuffix))\n",
    "\n",
    "        saved_model = torch.load(self.modelfile + self.modelsuffix)\n",
    "        self.model.load_state_dict(saved_model['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(saved_model['optimizer_state_dict'])\n",
    "        epoch = saved_model['epoch']\n",
    "        loss = saved_model['loss']\n",
    "        self.unk = saved_model['unk']\n",
    "        self.word_to_ix = saved_model['word_to_ix']\n",
    "        self.tag_to_ix = saved_model['tag_to_ix']\n",
    "        self.ix_to_tag = saved_model['ix_to_tag']\n",
    "        self.model.eval()\n",
    "        decoder_output = []\n",
    "        for sent in tqdm.tqdm(input_data):\n",
    "            decoder_output.append(self.argmax(sent))\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run the Baseline Solution on dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the chunker.tar from below wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-07 21:07:51--  https://onedrive.live.com/download?cid=1ED3E57B6F717CEC&resid=1ED3E57B6F717CEC%21123632&authkey=AOBFktG3ATadiE0\n",
      "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
      "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nihy2w.by.files.1drv.com/y4mxtrAf6Pq8M4QTE3vAD0EzrGhwRdxgCQS1eZfB-SxfnUqnwLUJyS5asCpU3eJH20cLVwGAvuYTZC0gD0VViFHretsnaXMiETnZ2AiLc4Yq8EVAkxqDFxcmkwnkU8DqCL1kQi2VwScoregOKJAi8PCN_0TRmfu0gbo3hoKbnv8NiV3hFLIrzQZyg0TrXNfh0BWobV-wgwFG2-872k-0w4vLw/chunker77.11.tar?download&psid=1 [following]\n",
      "--2019-11-07 21:07:52--  https://nihy2w.by.files.1drv.com/y4mxtrAf6Pq8M4QTE3vAD0EzrGhwRdxgCQS1eZfB-SxfnUqnwLUJyS5asCpU3eJH20cLVwGAvuYTZC0gD0VViFHretsnaXMiETnZ2AiLc4Yq8EVAkxqDFxcmkwnkU8DqCL1kQi2VwScoregOKJAi8PCN_0TRmfu0gbo3hoKbnv8NiV3hFLIrzQZyg0TrXNfh0BWobV-wgwFG2-872k-0w4vLw/chunker77.11.tar?download&psid=1\n",
      "Resolving nihy2w.by.files.1drv.com (nihy2w.by.files.1drv.com)... 13.107.42.12\n",
      "Connecting to nihy2w.by.files.1drv.com (nihy2w.by.files.1drv.com)|13.107.42.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5659678 (5.4M) [application/octet-stream]\n",
      "Saving to: ‘chunker77.11.tar’\n",
      "\n",
      "chunker77.11.tar    100%[===================>]   5.40M  14.4MB/s    in 0.4s    \n",
      "\n",
      "2019-11-07 21:07:54 (14.4 MB/s) - ‘chunker77.11.tar’ saved [5659678/5659678]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate -O chunker77.11.tar \"https://onedrive.live.com/download?cid=1ED3E57B6F717CEC&resid=1ED3E57B6F717CEC%21123632&authkey=AOBFktG3ATadiE0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we get the chunker file in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:02<00:00, 364.99it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('../data', 'train.txt.gz'), 'chunker77.11', '.tar')\n",
    "decoder_output = chunker.decode('../data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate the baseline Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the result with the reference dev.out. Use the functions provides in conlleval.py. (As we encounter some issues of importing the colleval module, we directly copied the code into the below section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: https://github.com/sighsmile/conlleval\n",
    "Modified by: Anoop Sarkar (anoopsarkar.github.io)\n",
    "\n",
    "This script applies to IOB2 or IOBES tagging scheme.\n",
    "If you are using a different scheme, please convert to IOB2 or IOBES.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys, re\n",
    "from collections import defaultdict\n",
    "\n",
    "def split_tag(chunk_tag):\n",
    "    \"\"\"\n",
    "    split chunk tag into IOBES prefix and chunk_type\n",
    "    e.g. \n",
    "    B-PER -> (B, PER)\n",
    "    O -> (O, None)\n",
    "    \"\"\"\n",
    "    if chunk_tag == 'O':\n",
    "        return ('O', None)\n",
    "    return chunk_tag.split('-', maxsplit=1)\n",
    "\n",
    "def is_chunk_end(prev_tag, tag):\n",
    "    \"\"\"\n",
    "    check if the previous chunk ended between the previous and current word\n",
    "    e.g. \n",
    "    (B-PER, I-PER) -> False\n",
    "    (B-LOC, O)  -> True\n",
    "\n",
    "    Note: in case of contradicting tags, e.g. (B-PER, I-LOC)\n",
    "    this is considered as (B-PER, B-LOC)\n",
    "    \"\"\"\n",
    "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
    "    prefix2, chunk_type2 = split_tag(tag)\n",
    "\n",
    "    if prefix1 == 'O':\n",
    "        return False\n",
    "    if prefix2 == 'O':\n",
    "        return prefix1 != 'O'\n",
    "\n",
    "    if chunk_type1 != chunk_type2:\n",
    "        return True\n",
    "\n",
    "    return prefix2 in ['B', 'S'] or prefix1 in ['E', 'S']\n",
    "\n",
    "def is_chunk_start(prev_tag, tag):\n",
    "    \"\"\"\n",
    "    check if a new chunk started between the previous and current word\n",
    "    \"\"\"\n",
    "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
    "    prefix2, chunk_type2 = split_tag(tag)\n",
    "\n",
    "    if prefix2 == 'O':\n",
    "        return False\n",
    "    if prefix1 == 'O':\n",
    "        return prefix2 != 'O'\n",
    "\n",
    "    if chunk_type1 != chunk_type2:\n",
    "        return True\n",
    "\n",
    "    return prefix2 in ['B', 'S'] or prefix1 in ['E', 'S']\n",
    "\n",
    "\n",
    "def calc_metrics(tp, p, t, percent=True):\n",
    "    \"\"\"\n",
    "    compute overall precision, recall and FB1 (default values are 0.0)\n",
    "    if percent is True, return 100 * original decimal value\n",
    "    \"\"\"\n",
    "    precision = tp / p if p else 0\n",
    "    recall = tp / t if t else 0\n",
    "    fb1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "    if percent:\n",
    "        return 100 * precision, 100 * recall, 100 * fb1\n",
    "    else:\n",
    "        return precision, recall, fb1\n",
    "\n",
    "\n",
    "def count_chunks(true_seqs, pred_seqs):\n",
    "    \"\"\"\n",
    "    true_seqs: a list of true tags\n",
    "    pred_seqs: a list of predicted tags\n",
    "\n",
    "    return: \n",
    "    correct_chunks: a dict (counter), \n",
    "                    key = chunk types, \n",
    "                    value = number of correctly identified chunks per type\n",
    "    true_chunks:    a dict, number of true chunks per type\n",
    "    pred_chunks:    a dict, number of identified chunks per type\n",
    "\n",
    "    correct_counts, true_counts, pred_counts: similar to above, but for tags\n",
    "    \"\"\"\n",
    "    correct_chunks = defaultdict(int)\n",
    "    true_chunks = defaultdict(int)\n",
    "    pred_chunks = defaultdict(int)\n",
    "\n",
    "    correct_counts = defaultdict(int)\n",
    "    true_counts = defaultdict(int)\n",
    "    pred_counts = defaultdict(int)\n",
    "\n",
    "    prev_true_tag, prev_pred_tag = 'O', 'O'\n",
    "    correct_chunk = None\n",
    "\n",
    "    for true_tag, pred_tag in zip(true_seqs, pred_seqs):\n",
    "        if true_tag == pred_tag:\n",
    "            correct_counts[true_tag] += 1\n",
    "        true_counts[true_tag] += 1\n",
    "        pred_counts[pred_tag] += 1\n",
    "\n",
    "        _, true_type = split_tag(true_tag)\n",
    "        _, pred_type = split_tag(pred_tag)\n",
    "\n",
    "        if correct_chunk is not None:\n",
    "            true_end = is_chunk_end(prev_true_tag, true_tag)\n",
    "            pred_end = is_chunk_end(prev_pred_tag, pred_tag)\n",
    "\n",
    "            if pred_end and true_end:\n",
    "                correct_chunks[correct_chunk] += 1\n",
    "                correct_chunk = None\n",
    "            elif pred_end != true_end or true_type != pred_type:\n",
    "                correct_chunk = None\n",
    "\n",
    "        true_start = is_chunk_start(prev_true_tag, true_tag)\n",
    "        pred_start = is_chunk_start(prev_pred_tag, pred_tag)\n",
    "\n",
    "        if true_start and pred_start and true_type == pred_type:\n",
    "            correct_chunk = true_type\n",
    "        if true_start:\n",
    "            true_chunks[true_type] += 1\n",
    "        if pred_start:\n",
    "            pred_chunks[pred_type] += 1\n",
    "\n",
    "        prev_true_tag, prev_pred_tag = true_tag, pred_tag\n",
    "    if correct_chunk is not None:\n",
    "        correct_chunks[correct_chunk] += 1\n",
    "\n",
    "    return (correct_chunks, true_chunks, pred_chunks, \n",
    "        correct_counts, true_counts, pred_counts)\n",
    "\n",
    "def get_result(correct_chunks, true_chunks, pred_chunks,\n",
    "    correct_counts, true_counts, pred_counts, verbose=True):\n",
    "    \"\"\"\n",
    "    if verbose, print overall performance, as well as preformance per chunk type;\n",
    "    otherwise, simply return overall prec, rec, f1 scores\n",
    "    \"\"\"\n",
    "    # sum counts\n",
    "    sum_correct_chunks = sum(correct_chunks.values())\n",
    "    sum_true_chunks = sum(true_chunks.values())\n",
    "    sum_pred_chunks = sum(pred_chunks.values())\n",
    "\n",
    "    sum_correct_counts = sum(correct_counts.values())\n",
    "    sum_true_counts = sum(true_counts.values())\n",
    "\n",
    "    nonO_correct_counts = sum(v for k, v in correct_counts.items() if k != 'O')\n",
    "    nonO_true_counts = sum(v for k, v in true_counts.items() if k != 'O')\n",
    "\n",
    "    chunk_types = sorted(list(set(list(true_chunks) + list(pred_chunks))))\n",
    "\n",
    "    # compute overall precision, recall and FB1 (default values are 0.0)\n",
    "    prec, rec, f1 = calc_metrics(sum_correct_chunks, sum_pred_chunks, sum_true_chunks)\n",
    "    res = (prec, rec, f1)\n",
    "    if not verbose:\n",
    "        return res\n",
    "\n",
    "    # print overall performance, and performance per chunk type\n",
    "    \n",
    "    print(\"processed %i tokens with %i phrases; \" % (sum_true_counts, sum_true_chunks), end='')\n",
    "    print(\"found: %i phrases; correct: %i.\\n\" % (sum_pred_chunks, sum_correct_chunks), end='')\n",
    "        \n",
    "    print(\"accuracy: %6.2f%%; (non-O)\" % (100*nonO_correct_counts/nonO_true_counts))\n",
    "    print(\"accuracy: %6.2f%%; \" % (100*sum_correct_counts/sum_true_counts), end='')\n",
    "    print(\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\" % (prec, rec, f1))\n",
    "\n",
    "    # for each chunk type, compute precision, recall and FB1 (default values are 0.0)\n",
    "    for t in chunk_types:\n",
    "        prec, rec, f1 = calc_metrics(correct_chunks[t], pred_chunks[t], true_chunks[t])\n",
    "        print(\"%17s: \" %t , end='')\n",
    "        print(\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\" %\n",
    "                    (prec, rec, f1), end='')\n",
    "        print(\"  %d\" % pred_chunks[t])\n",
    "\n",
    "    return res\n",
    "    # you can generate LaTeX output for tables like in\n",
    "    # http://cnts.uia.ac.be/conll2003/ner/example.tex\n",
    "    # but I'm not implementing this\n",
    "\n",
    "def evaluate(true_seqs, pred_seqs, verbose=True):\n",
    "    (correct_chunks, true_chunks, pred_chunks,\n",
    "        correct_counts, true_counts, pred_counts) = count_chunks(true_seqs, pred_seqs)\n",
    "    result = get_result(correct_chunks, true_chunks, pred_chunks,\n",
    "        correct_counts, true_counts, pred_counts, verbose=verbose)\n",
    "    return result\n",
    "\n",
    "def evaluate_conll_file(fh):\n",
    "    true_seqs, pred_seqs = [], []\n",
    "    \n",
    "    for line in fh:\n",
    "        cols = line.strip().split()\n",
    "        # each non-empty line must contain >= 3 columns\n",
    "        if not cols:\n",
    "            true_seqs.append('O')\n",
    "            pred_seqs.append('O')\n",
    "        elif len(cols) < 3:\n",
    "            raise IOError(\"conlleval: too few columns in line %s\\n\" % line)\n",
    "        else:\n",
    "            # extract tags from last 2 columns\n",
    "            true_seqs.append(cols[-2])\n",
    "            pred_seqs.append(cols[-1])\n",
    "    return evaluate(true_seqs, pred_seqs)\n",
    "\n",
    "def read_file(handle):\n",
    "    contents = re.sub(r'\\n\\s*\\n', r'\\n\\n', handle.read())\n",
    "    contents = contents.rstrip()\n",
    "    return contents.split('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the evaluate function to see the FB1 score for the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11930 phrases; correct: 9186.\n",
      "accuracy:  86.95%; (non-O)\n",
      "accuracy:  87.91%; precision:  77.00%; recall:  77.22%; FB1:  77.11\n",
      "             ADJP: precision:  45.56%; recall:  18.14%; FB1:  25.95  90\n",
      "             ADVP: precision:  68.38%; recall:  46.73%; FB1:  55.52  272\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  75.38%; recall:  80.52%; FB1:  77.87  6662\n",
      "               PP: precision:  91.37%; recall:  88.45%; FB1:  89.88  2363\n",
      "              PRT: precision:  70.27%; recall:  57.78%; FB1:  63.41  37\n",
      "             SBAR: precision:  86.29%; recall:  45.15%; FB1:  59.28  124\n",
      "               VP: precision:  69.06%; recall:  71.40%; FB1:  70.21  2382\n"
     ]
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('../data','reference','dev.out')) as r:\n",
    "    for sent in read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "    evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Improvement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Shuffle the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tried to shuffle the training data to see if there is an improvement for generalization. We modify the train function inside class LSTMTagger by adding a shuffling fucntion inside every epoch:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for epoch in range(self.epochs):\n",
    "    random.shuffle(self.training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement is not significant. So we tried another appraoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modify Loss Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the FB1 score, we try to increase the loss value multiple by a value scale_factor (where scale_factor is set to 2.0 below)\n",
    "\n",
    "Inside the class LSTMTagger->train funtion, step4: Compute the loss, gradients, and update the parameters by calling optimizer.step(), we scale the loss function as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "        scale_factor = 2.0\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            # random.shuffle(generate_noise(self.training_data))\n",
    "            for sentence, tags in tqdm.tqdm(self.training_data):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "\n",
    "                # Step 2.1. Create character level vector representation of size n x 300\n",
    "                char_vectors = create_char_level_vectors(sentence, self.word_to_ix, self.char_to_ix)\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = self.model(sentence_in, char_vectors)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets) * scale_factor\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                    }, savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Run the Improved Solution on dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the chunker78.73.tar for this result from below wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-07 21:10:30--  https://onedrive.live.com/download?cid=1ED3E57B6F717CEC&resid=1ED3E57B6F717CEC%21123630&authkey=AMc9o1GHbrKlywA\n",
      "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
      "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ampbiq.by.files.1drv.com/y4mWhvVxS-IhruHigEm7G-aXRy1V54DPYLu2ufQEcKrkJD4llaqtSaDS2rsimRV_r45NAA3vyA9QUapXsAffrwC7O6yTdz-U6XejSlNILDC87eR5-1QBzngZGPmhD9s1gc72bK9DmUhRbQqUCWiEOaDmW4AUzfPQXgYQdpITzjO3lJ27dSB79O3M4_J23ZJgCtBcbcTR99jbKhZlZKBJF62yQ/chunker78.72.tar?download&psid=1 [following]\n",
      "--2019-11-07 21:10:31--  https://ampbiq.by.files.1drv.com/y4mWhvVxS-IhruHigEm7G-aXRy1V54DPYLu2ufQEcKrkJD4llaqtSaDS2rsimRV_r45NAA3vyA9QUapXsAffrwC7O6yTdz-U6XejSlNILDC87eR5-1QBzngZGPmhD9s1gc72bK9DmUhRbQqUCWiEOaDmW4AUzfPQXgYQdpITzjO3lJ27dSB79O3M4_J23ZJgCtBcbcTR99jbKhZlZKBJF62yQ/chunker78.72.tar?download&psid=1\n",
      "Resolving ampbiq.by.files.1drv.com (ampbiq.by.files.1drv.com)... 13.107.42.12\n",
      "Connecting to ampbiq.by.files.1drv.com (ampbiq.by.files.1drv.com)|13.107.42.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5659687 (5.4M) [application/octet-stream]\n",
      "Saving to: ‘chunker78.73.tar’\n",
      "\n",
      "chunker78.73.tar    100%[===================>]   5.40M  13.1MB/s    in 0.4s    \n",
      "\n",
      "2019-11-07 21:10:31 (13.1 MB/s) - ‘chunker78.73.tar’ saved [5659687/5659687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate -O chunker78.73.tar \"https://onedrive.live.com/download?cid=1ED3E57B6F717CEC&resid=1ED3E57B6F717CEC%21123630&authkey=AMc9o1GHbrKlywA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we get the chunkerLoss file in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:02<00:00, 353.75it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('../data', 'train.txt.gz'), 'chunker78.73', '.tar')\n",
    "decoder_output = chunker.decode('../data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Evaluate the improved Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11986 phrases; correct: 9401.\n",
      "accuracy:  87.81%; (non-O)\n",
      "accuracy:  88.69%; precision:  78.43%; recall:  79.03%; FB1:  78.73\n",
      "             ADJP: precision:  46.34%; recall:  25.22%; FB1:  32.66  123\n",
      "             ADVP: precision:  66.25%; recall:  53.27%; FB1:  59.05  320\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  77.02%; recall:  81.74%; FB1:  79.31  6619\n",
      "               PP: precision:  92.81%; recall:  88.32%; FB1:  90.51  2323\n",
      "              PRT: precision:  77.78%; recall:  62.22%; FB1:  69.14  36\n",
      "             SBAR: precision:  78.62%; recall:  48.10%; FB1:  59.69  145\n",
      "               VP: precision:  71.74%; recall:  75.35%; FB1:  73.50  2420\n"
     ]
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('../data','reference','dev.out')) as r:\n",
    "    for sent in read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "    evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By scaling the loss funtion, wo have reached a FB1 score of 78.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Try with Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the score, we have implement a data aggergation function.\n",
    "The function is simple which randomly swap the character and drop a character from the words of the sentense. However, the final result does not change much after the implementation, accuary on dev is similar to section 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shuffle(training_data, random_seed = 0):\n",
    "    new_training_data = []\n",
    "    for sentence, tags in training_data:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            if(len(word) >= 3):\n",
    "                random.seed(random_seed) # Adding seed making sure get the same result for next time with same training dataset\n",
    "                rand = random.randint(0, 2)\n",
    "                random_seed += 1\n",
    "\n",
    "                random.seed(random_seed)\n",
    "                target_index = random.randint(1, len(word)-2)\n",
    "                random_seed += 1\n",
    "\n",
    "                # Swap\n",
    "                if rand == 2:\n",
    "                    new_word = word[:target_index] + word[target_index + 1] + word[target_index] \n",
    "                # Drop  \n",
    "                if rand == 1:\n",
    "                    new_word = word[:target_index] + word[target_index+1:]\n",
    "                # Normal\n",
    "                if rand == 0:\n",
    "                    new_word = word\n",
    "            else:\n",
    "                    new_word = word\n",
    "            new_sentence.append(new_word)\n",
    "            # adding noise to the word in sentence\n",
    "        new_training_data.append((new_sentence, tags))\n",
    "    random.shuffle(new_training_data)\n",
    "    return new_training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LSTMTagger, we use the data_shuffle() inside the LSTMTagger train() to shuffle and add noise on training\n",
    "We referenced the new train() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "        lamb = 2.5\n",
    "\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            training_data = data_shuffle(self.training_data)\n",
    "            for sentence, tags in tqdm.tqdm(training_data):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                \n",
    "                # get character level vector\n",
    "                sentence_char_in = prepare_char_sequence(sentence, self.word_to_char_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "\n",
    "                # Step 3. Run our forward pass. calling forward in LSTMTaggerModel above\n",
    "                tag_scores = self.model((sentence_in, sentence_char_in))\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets) * lamb\n",
    "                logging.info(\"loss:\", loss)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                    }, savefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, incooperating data aggregation to our model does not offer any performance improvement, which produced 78.73 accuary on dev data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we aim to find non-recursive syntactic groups of words.\n",
    "We first implement the Baseline solution with semi-Character RNN idea with phrasal chunker and reach a FB1 score of 77.1.\n",
    "\n",
    "Then, we improve the solution by scaling the loss function, since we expects that the loss penalty is not sufficient enough in only 10 epoches. After scaling the loss, the accuary of dev dataset has improved significaly from 77.1 to 78.73.\n",
    "\n",
    "Also, We tried to further improve our score using data aggregation method. However, the result does not improve the score, which produce 78.73 as the scaling methond.\n",
    "\n",
    "To sum up, scaling the loss function in the Baseline solution with semi-Character RNN idea with phrasal chunker will allows us to achieve 78.73."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
